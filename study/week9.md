Week 9 (01/03 ~ 01/05)
===
>  ##### 주간 요약
>  Level 2 시작과 함께 DKT 프로젝트 대회가 시작되었다.  
>  새로운 팀원들을 만나 인사하는 시간을 가졌다.  
>  NLP 외의 분야에서 Transformer를 활용하는 법에 대해 배웠다.  

Day 40 (01/03)
---
여러 방면에서 시작을 맞이하는 날이다.  
Level 2가 시작되었고, DKT 프로젝트 대회가 시작되었고, 새로운 팀원들을 만났다.  
Level 2부터는 좀 더 프로젝트 위주의 과정이 될 예정이다. 이번주부터 약 4주간은 DKT 프로젝트, 그 뒤에는 영화 추천 프로젝트가 진행되는 듯하다.  

DKT란 Deep Learning과 Knowledge Tracing을 합한 단어다. 각 유저들의 문제 풀이 기록을 보면서 현재 지식 상태를 DL로 모델링하는 것이다.  
이번 DKT 대회에서는 일부 유저들의 문제 풀이 기록 중 마지막 문제에 대한 정답 여부를 예측하는 것이 목표다.  
이전 대회처럼 처음 며칠 동안은 프로젝트 관련 강의를 듣고, 그 뒤에 본격적으로 프로젝트 활동이 시작된다.  

오늘 강의에서는 DKT가 어떤 문제인지와 주어진 데이터에 대한 간단한 EDA를 다뤘다.  
이전 대회와 달리 활용 가능한 feature가 적어서 EDA 분량도 많지 않았다. 아마 전처리를 통해 추가적인 정보들을 뽑아내는 작업이 필요해 보인다.  

강의를 듣고 나서는 새로운 팀원들과 첫 피어세션을 진행했다.  
내일 팀 소개 시간이 있을 예정이라 발표 자료를 만들며 자기소개를 하고, 그라운드 룰을 정했다.  
우리 조는 총 5명인데, 가장 큰 특징은 5명 모두 내향형인 I이고 5명의 전공이 다 다르다는 것이다.  
캠프 내 비전공자 비율이 그렇게 높지 않을 것 같은데, 우리 조에 유독 많이 몰린 거 같아서 신기했다.  

아직 첫 날이다 보니 서로 어색한데, 그래도 스페셜 피어세션 때 뵌 분도 있고 각자 캠프 생활에 어느 정도 익숙해져서 level 1을 시작할 때보다는 분위기가 덜 딱딱했다.  
이번 조원들과도 서로 마음이 잘 맞아서 즐겁게 활동할 수 있길 바라본다.  

+ ##### 키워드: Level 2 시작, DKT 프로젝트 대회 시작, 팀 결성

Day 41 (01/04)
---
오늘은 강의와 오피스아워를 통해 베이스라인 코드에 대한 해설을 들었다.  
Data leakage를 막기 위해 user 단위로 split을 했다는 점, DKT를 sequential 데이터에 대한 문제로 간주하기 위해 각 유저들의 풀이 기록을 하나의 sequence로 간주했다는 점 등을 알았다.  
그리고 몇몇 다른 대회들의 solution도 소개해주셨는데, 약물 효과 예측 문제를 conv1D로 접근한 사례가 인상적이었다.  
feature들을 모두 concat. 한 뒤 dense layer를 한 번 통과시키고 이 벡터를 여러 channel을 갖는 1D 벡터로 변형해 conv1D를 적용시켰다.  
다만 이번 대회에서는 feature 수가 적어서 별로 효과적이지 않을 것 같기도 하다. Embedding을 통해 차원을 늘리는 방법도 있겠으나, embedding 차원을 너무 키우는 것이 오히려 overfitting을 유발할 것 같다.  

피어세션 때는 앞으로 프로젝트를 진행하기에 앞서 github 활용 방법 등을 먼저 정하고 시작하자는 의견이 나왔다.  
지난 프로젝트에서는 github를 제대로 활용하지 못해서 좀 아쉬웠는데, 이제는 최종 프로젝트까지 함께 할 팀이니까 미리 협업 방식을 잘 정리해두는 게 좋을 듯하다.  

+ ##### 키워드: 베이스라인 코드 해설, 협업 컨벤션

Day 42 (01/05)
---
오늘은 NLP 문제가 아닐 때 transformer를 어떻게 활용하는지, transformer를 활용한 대회 solution에는 어떤 것들이 있는지, 그리고 feature engineering 아이디어로 활용할 만한 것들에 대해 배웠다.  

정형 데이터를 transformer에 활용하려면, 각 feature들을 embedding 한 후 concat하여 특정 시점의 값으로 사용해야 한다.  
예를 들어 t 시점의 정보에 대해, feature들을 embedding 후 concat 하고, 이렇게 얻어진 vector들을 여러 시점의 값들을 모아 transformer에 넣는 것이다.  
이때 feature가 연속형이라면 단순히 linear layer를 거치면 되고, 범주형이라면 각 범주를 숫자로 labeling 후 embedding 해야 한다.  

Transformer에서 sequence 길이를 L, embedding 차원을 d라고 하면 시간복잡도는 O(L**2 X d)가 된다.  
일반적으로 sequence 길이가 길어질수록 transformer의 성능이 올라가지만, 시간복잡도와 메모리의 한계를 마주하게 된다고 한다.  
다른 DKT 대회에서는 이 문제를 Last Query Transformer RNN이라는 방법을 통해 해결했는데, transformer에서 마지막 데이터의 query만을 사용하는 것이다.  
원래는 모든 입력 데이터에 대해 query를 만들기에 query 행렬이 (L X d) 차원을 갖는데, 마지막 데이터의 query만 사용하면 (1 X d)가 된다.  
이로 인해 시간복잡도도 O(L X d)로 줄어드는 효과를 갖는다.  
마지막 query만을 쓰는 것은 각 sequence마다 마지막 문제의 정답을 맞추는 것을 목표로 하기 때문에, 마지막 문제와 다른 문제들과의 연관성만을 학습하면 된다고 판단했기 때문이라 한다.  
한 가지 의아한 점은, output으로 기존 sequence와 같은 길이를 갖는 행렬을 반환하여 LSTM에 넘겨준다는 점인데, 여기서 왜 기존 sequence와 같은 길이가 되는지 잘 모르겠다.  
원래 transformer의 구조를 생각하면 query가 1개이므로 output도 (1, batch size, hidden dimension)의 크기가 되어야 할 것이다.  

Github에서 소스 코드를 찾아 읽어봤는데, skip connection을 할 때 차원이 복구되는 것 같다.  
Attention을 취하기 전의 (seq_len, batch_size, hidden_dim) 크기의 tensor가 있고, attention 후에는 (1, batch_size, hidden_dim)의 형태가 될 것이다.  
이 둘을 skip connection을 위해 더해주면 broadcasting에 의해 연산이 되고, 최종적으로 (seq_len, batch_size, hidden_dim)의 형태가 되는 것이다.  

강의를 듣느라 프로젝트 관련해서는 아직 손을 못 대고 있는데, 데일리 스크럼과 피어세션을 통해 아이디어 공유를 위주로 진행 중이다.  
Conv 1D나 MF 적용해보기, 대분류별로 모델을 따로 학습 후 앙상블 해 보기, 문제 복습 횟수를 feature로 만들어보기 등등 여러 아이디어를 제시했다.  
그 중 '최근 데이터 외에 이전 데이터도 활용하기'라는 아이디어도 있었는데, t~t+n 시점의 데이터로 t+n+1 시점의 데이터를 예측하는 것이다.  
현재 베이스라인 코드에는 가장 최근의 n개 데이터만으로 마지막 문제에 대한 정답 여부를 예측하는데, t를 옮기며 여러 데이터를 모두 활용하자는 아이디어였다.  
나는 단순하게 t를 1씩 바꿔서 적용하는 것만을 생각했는데, 아이디어 공유 과정에서 비슷한 데이터를 너무 많이 학습하게 된다는 지적이 나왔다.  
나도 그 의견에 동의해서 그럼 전체 시기의 데이터 중 n개를 sampling 해서 사용하면 데이터 증강 효과도 볼 수 있지 않을까? 라는 아이디어를 추가로 제시했다.  
의견 공유를 하는 과정에서 아이디어가 발전되는 게 느껴져서 집단 지성의 장점을 체감할 수 있었다.  

+ ##### 키워드: Transformer 활용, Last Query Transformer RNN, 아이디어 공유
