Week 7 (12/18 ~ 12/22)
===
>  ##### 주간 요약
>  

Day 31 (12/18)
---
오늘은 프로젝트를 위해 앙상블 함수를 구현했다.  
베이스라인 코드로 제공된 앙상블 기법만으로는 부족한 감이 있어서 새로운 기법을 활용해서 함수를 만들어봤다.  

다른 조원들이 좋은 결과를 낸 모델들이 있어서 해당 모델들을 내 기법으로 앙상블 했더니 결과가 조금 더 개선되었다.  
피어세션 때 이에 대해 얘기를 나눠봤는데, 일단 나는 앙상블 기법을 발전시키는 방향으로 더 시도해보고자 한다.  

앙상블 기법을 어떻게 구현했는지, 수식을 어떻게 활용했는지 등등 적을 만한 것들이 좀 더 있는데, 팀 프로젝트 경쟁이다보니 유출이 될 수 있을 듯해 생략했다.  
컨디션이 아직 완전히 회복되지 않았다는 문제가 남아있기도 해서, 오늘은 짧게 적고 좀 더 쉬어야겠다.  

+ ##### 키워드: 앙상블 기법 구현

Day 32 (12/19)
---
오늘은 앙상블 기법을 바꿔서 어제보다 성적을 조금 더 향상시켰다.  
등수는 전체 8등 중 3등으로 유지 중인데, 평가 수치인 RMSE를 어제보다 조금 더 줄였다.  
어제와 마찬가지로 조원들이 각자 모델을 조금씩 더 발전시켜오고, 나는 앙상블에 대해 더 고민해보기로 했다.  
슬슬 앙상블로 성능을 개선하는 데에는 한계가 있는 듯해서 기본 모델 성능의 향상이 필요한 시점인 것 같다.  

오피스아워에서는 온라인 테스트에 대한 내용을 다뤘다.  
실험군을 어떻게 나눌 것인가, 개선할 지표와 가드레일을 무엇으로 설정할 것인가, 그룹을 잘 나눴는지 어떻게 확인할 것인가 등등에 대한 얘기를 들었다.  

오피스아워 이후 멘토링에서는 프로젝트에 대한 조언들을 들었다.  
어떤 시도들을 해 볼 수 있을지, 실험은 어떤 순서로 해 봐야 할지 등을 얘기해주셨다.  

+ ##### 키워드: 온라인 테스트

Day 33 (12/20)
---
'simple is the best'라는 격언을 되새긴 날이었다.  
그동안 여러 앙상블 기법을 고안해보면서 테스트를 해 왔는데, 오히려 단순하게 결과값을 평균한 것이 제일 결과가 좋았다.  
그래서 다른 앙상블 기법을 적용해 볼 필요성이 좀 떨어지기도 했고, 현재 모델 결과들로는 앙상블로 성능을 더 높이기 어려워 보여서 하이퍼 파라미터 튜닝을 시작했다.  

튜닝을 하다 보니 유독 batch size에 valid loss가 크게 영향을 받았다.  
그래서 이에 대해 검색해봤는데, batch size가 작아질수록 loss의 분산이 커지고 다양한 데이터를 학습해서 regularization이 잘 되는 장점이 있다고 한다.  
대신 step이 많아지기 때문에 local optima에 빠지기 쉬운 단점이 있다고 한다.  
내가 튜닝을 해봤을 때는 batch size를 줄이면 valid loss가 계속 줄어들기만 했는데, 어쩌면 이번 대회의 데이터에 대해 global optima와 local optima가 비슷한 게 아닐까 싶기도 하다.  
그런데 이와는 별개로 valid dataset의 batch size가 줄어들어서 valid loss가 줄어드는 게 아닐까 싶은 생각도 들었다.  
각 예측의 error가 똑같아도 100개씩 묶어서 RMSE를 구한 후 평균을 구한 것과, 10000개씩 묶어서 RMSE를 구한 후 평균을 구한 것의 결과값이 다르기 때문이다.  
현재 베이스라인 코드에서는 batch_size 파라미터를 조정하면 train dataset과 valid dataset의 batch size가 동시에 조정된다.  
그래서 train dataset의 batch size를 고정하고 valid dataset만 바꿔봤는데, valid loss가 같게 나왔다.  
이를 보면 batch size를 줄이는 것이 모델의 예측 성능 자체를 높여준 게 맞는 듯하다.  

valid dataset 외에 test dataset에 대해서도 확인해보고 싶었는데, 오늘의 리더보드 제출 횟수를 다 써서 확인을 못 해 봤다.  
피어세션 때 이에 대해 회의했는데 일단 하이퍼 파라미터 튜닝을 더 해 보고 내일 결과를 확인하기로 했다.  
그 외에 책의 summary 정보를 읽어서 bert를 이용해 embedding으로 변환 후 예측에 활용하는 접근도 진행 중인데, 팀에서 다른 조원 분들이 맡아주시기로 해서 마찬가지로 내일 결과를 보기로 했다.  

이론 공부를 할 때와 달리 프로젝트를 시작하니 왠지 AI와 개발자 취업 등에 대해 회의감이 많이 들었다.  
이전에는 적절한 논리를 통해 모델을 설계하고 튜닝하면 성능이 잘 나올 거라는 믿음이 있었는데, 막상 프로젝트를 시작하니 단순한 접근들이 더 성과가 좋아서 그런 것 같다.  
다른 조원 분도 비슷한 고민을 하고 계신다고 해서 '나만 이렇게 생각하는 게 아니구나' 라는 생각이 들었다.  
내일이면 프로젝트도 마무리 되니까 조원 분들께 폐를 끼치지 않도록 조금만 더 힘내서 잘 마무리해야겠다.  

+ ##### 키워드: 하이퍼 파라미터 튜닝, batch size, 진로
