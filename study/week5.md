Week 5 (12/4 ~ 12/8)
===
>  ##### 주간 요약
>  RecSys에서 활용되는 Deep Learning 모델들과 MAB에 대해 배웠다.  
>  최종 프로젝트 주제에 대한 고민을 시작했다.  
>  내가 잘 학습해나가고 있다는 자신감을 얻었다.  

Day 21 (12/4)
---
지난주에 이어 RecSys 이론 강의를 듣는 주간이다.  
전체 10강 중 6강을 지난주에 들었기에 이번주는 비교적 강의가 널널할 것 같다.  

보통은 아침에 데일리스크럼을 할 때 당일 학습 목표량을 정하고 이를 공유하는데, 오늘은 프로젝트 주제에 대한 이야기를 추가로 나눴다.  
나는 주말동안 2가지 주제를 생각했는데, 하나는 당뇨병 환자를 위한 식단 추천이고 나머지는 개인 취향을 반영한 배경화면 이미지 생성기다.  
다른 조원들도 컴퓨터를 켜면 어떤 활동을 할지 추천해주는 프로그램과 취미 활동을 추천해주는 서비스 등을 제안했다.  
현재 조가 그대로 유지될지도 아직 미지수고, 파이널 프로젝트까지 기간도 많이 남아서 주제는 천천히 좀 더 고민해보기로 했다.  

강의 내용은 Graph Neural Network(GNN)에 대한 내용으로 시작했다.  
정형 데이터나 이미지와 달리 유저-아이템의 관계는 Graph를 통해 Non-Euclidean Space로 표현하는 것이 유리하다.  
Neural Graph Collaborative Filtering(NGCF)은 이런 graph 구조를 활용한 모델이다.  
각 유저와 아이템을 임베딩한 후, 이들로 Message를 만든다. 한 유저에 대한 어떤 아이템의 Message는 두 임베딩의 element-wise product와 아이템의 임베딩에 가중치를 곱해 만들어진다.  
그 후 각 유저 혹은 아이템별로 연결된 노드들과의 Message를 모두 합해 activation 함수를 적용함으로써 새로운 임베딩을 만든다.  
이렇게 만들어진 임베딩들은 다음 레이어에서 다시 Message를 만드는데 활용되어 재귀적인 구조를 형성한다.  
총 l개의 레이어에 대해 만들어진 e_0, ... , e_u를 모두 concatenate하여 최종 임베딩 e*를 만들고, 유저의 e와 아이템의 e를 곱해 유저와 아이템간의 선호도를 예측한다.  

LightGCN은 NGCF를 단순화한 모델로, 한 노드에 연결된 노드들의 임베딩을 단순히 합한 후 scale을 조정하여 다음 레이어의 임베딩으로 활용한다.  
대신 각 layer의 임베딩을 concat.하지 않고 가중합을 함으로써 가까운 노드들을 더 크게 반영한다.  

RNN을 활용한 GRU4Rec 모델은 유저의 선호가 계속 변한다는 가정을 바탕으로, 이전 선호들을 통해 현재의 선호를 추측한다.  
본래 RNN에서 사용되던 GRU를 활용한 것이 특징이다.  

그 후에는 Context-aware Recommendation(CAR)에 대해 배웠다.  
기존에 배웠던 모델들은 대부분 유저-아이템 상호작용만을 활용했는데, CAR은 이에 더해 맥락적 정보를 추가로 고려하는 것이다.  
대표적으로는 Factorization Machine(FM)과 Field-aware Factorization Model(FFM)이 있다.  
FM은 하나의 선택에 대한 n개의 feature를 바탕으로, linear regression에 feature간의 상호작용을 더해준다.  
이때 n개의 feature는 각각 v라는 vector를 갖고, feature간의 상호작용은 두 v의 dot product를 가중치로 하여 더해진다.  
FFM은 여기서 여러 feature들을 하나의 field로 묶고, 각 feature별로 field들에 대한 상호작용을 v로 갖는다.  
따라서 하나의 feature는 f개의 field에 대해 총 f개의 v를 갖는다.  
대체로 FM과 FFM이 단순한 linear regression이나 모든 상호작용을 개별적으로 생각하는 polynomial model에 비해 성능이 좋다고 한다.  
다만 FM과 FFM의 우위는 데이터셋이나 FFM에서 field를 어떻게 정의하느냐에 따라 달라진다고 한다.  

그 외에 weak learner들을 결합하여 decision tree를 만드는 Gradient Boosting Machine(GBM)에 대해서도 배웠다.  
Boosting 대신 Bagging을 쓰는 random forest보다는 성능이 좋지만, 학습 속도가 느리고 과적합이 심하다는 단점이 있다.  
때문에 XGBoost, LightGBM, CatBoost 등의 대안 모델들이 제시되었다.  

강의를 모두 듣고 과제도 해결한 후 피어세션을 진행했다.  
강의를 들으면서 FFM의 모든 field를 하나의 feature로만 구성하면 사실상 polynomial model이 될까? 라는 궁금증이 생겨서 이에 대해 조원들과 의견을 나눴다.  
내 의문을 제시하는 과정에서 FM과 FFM에 대해 내가 이해한 방식으로 설명했는데, 다른 조원들의 FM과 FFM에 대한 이해에 도움이 된 듯했다.  
설명을 마치고 내 궁금증을 얘기하니 다들 내 생각에 동의해주었다. 슬랙의 질문게시판에도 질문을 올려뒀는데, 내일 조교님이 답변을 달아주시면 확실히 해결될 것 같다.  

>  오늘의 질문 횟수: 4  
>  오늘의 답변 횟수: 1  

+ ##### 키워드: NGCF, LightGCN, FM, FFM

Day 22 (12/5)
---
오늘은 깃허브 특강 후반부가 진행되었다.  
지난주에 다뤘던 내용들을 복습하고, rebase, cherrypick, pull, push 등을 배웠다.  
예전에는 git으로 원격 저장소를 다룰 때 SourceTree를 썼는데, VS Code로 다루는 게 훨씬 편해서 갈아타기로 했다.  
특강 내용을 실습하며 따라갈 때 내 개인 repository와는 호환이 잘 되었는데, 강사님의 공유 repository에 접근하려니 자꾸 에러가 났다.  
아마 인증 관련해서 문제가 생긴 것 같은데, 특강이 끝나고 공유 repo가 사라져서 확인은 못 해 봤다.  
나중에 프로젝트가 시작되면 본격적으로 작업을 시작하기 전에 제대로 접근이 되는지 먼저 테스트해봐야겠다.  

특강을 마친 후에는 심화 과제를 살펴봤다. 가능하면 풀어보려고 했는데 슬랙 공지로 구인구팀 관련 공지가 날아와서 과제는 내일 마저 보기로 했다.  
슬랙 공지를 살펴보니 이제 Level 2부터 활동할 팀을 결성한다고 한다. 노션 페이지에 자기소개를 적어두고 슬랙이나 줌을 통해 팀을 만들어나가는 듯하다.  
나는 일단 현재 조원들과 같이 진행하기로 어느 정도 얘기가 되어 있어서 금요일에 조원들이 오프라인 행사를 다녀오면 마저 논의하고 결정해야겠다.  

>  오늘의 질문 횟수: 2  
>  오늘의 답변 횟수: 0  

+ ##### 키워드: Github, 구인구팀

Day 23 (12/6)
---
오늘은 CTR 예측을 위한 딥러닝 모델들과 Multi-Armed Bandit에 대해 배웠다.  
첫 모델은 Wide & Deep으로, memorization을 담당하는 Wide component와 generalization을 담당하는 Deep component를 결합하여 구성된다.  
이 때 wide와 deep component의 입력값이 다르고, wide component에서 feature engineering이 필요하다.  
이를 개선한 모델이 DeepFM 모델로, wide component를 FM으로 대체한 모델이다.  
FM component는 low-order interaction에 효과적이고, Deep component는 high-order interaction에 효과적이다.  

Deep Interest Network(DIN)는 user behavior feature를 처음 사용한 모델이다.  
기존 딥러닝 기반 모델들은 사용자의 다양한 관심사를 반영하기 어려웠다.  
반면 DIN은 유저가 기존에 소비한 아이템 리스트를 user behavior feature로 만들어 반영할 수 있다.  
Transformer의 attention과 유사한 Local Activation이라는 layer를 통해 소비한 아이템들과 후보 아이템간의 연관성을 계산하고, 이를 이용해 아이템 임베딩을 가중합한다.  

Behavior Sequence Transformer(BST)는 여기서 더 나아가 아이템 소비 이력을 sequential data로 간주해 순서까지 고려한다.  
DIN이 attention과 유사한 부분을 갖고 있다면, BST는 Transformer와 동일하게 multi-head self attention, add & norm, FFN 레이어를 모두 갖는다.  

Multi-Armed Bandit(MAB)은 각 아이템들을 하나의 슬롯 머신처럼 생각하고, 각 슬롯 머신마다 보상이 나올 확률이 정해져있다는 가정을 활용한다.  
이 때 어떤 슬롯 머신을 얼마나 많이 당겨야 보상을 최대화할 수 있는지에 대한 방법론이 MAB이다.  
가장 단순한 Greedy Algorithm, 랜덤성을 추가한 Epsilon-Greedy Algorithm, 기댓값에 신뢰도를 추가한 Upper Confidence Bound 등이 있다.  
좀 더 발전된 방법론으로는 베타 분포를 실시간으로 갱신해 활용하는 Thompson Sampling, 아이템을 선택할 때의 context vector를 활용하는 LinUCB 등이 있다.  

강의를 듣고 피어세션 때는 과제에 대해 질문 교환을 하고, 내일 스터디를 어떻게 할지 논의했다.  
한 주제로 스터디를 고정하기보다는 각자 공부하고 싶은 내용을 자유롭게 공부하고, 피어세션 때 간단히 공부한 내용을 설명하는 식으로 진행하게 됐다.  

피어세션 후에 멘토링 시간에는 CS 공부나 코테 준비는 어느 정도나 해야 할지, Front-end와 Back-end 능력은 얼마나 중요할지 등에 대해 들었다.  
CS는 네트워크 같은 분야보다는 자료구조 등 AI 개발과 관련된 분야를 위주로 공부하는 게 좋을 듯하고, FE나 BE 역시 상대적으로 AI 개발 능력에 비해 우선도가 낮은 듯하다.  
프로젝트 주제 선정에 대해서도 말씀하셨는데, 프로젝트 기획 의도가 무엇인지, 프로젝트를 진행할 때 데이터 처리나 모델 선정 등은 어떻게 할지 등을 고려해야 한다고 하셨다.  
특히 모델을 구현할 때 복잡하고 세련된 모델보다는 간단한 모델이라도 어떻게 서비스에 맞게 개량하는지가 더 중요하다.  
그리고 내가 생각했던 당뇨 환자 메뉴 추천 프로젝트도 코멘트를 간단히 받았는데, 데이터를 구하기가 어렵고 ML로 잘 해결할 수 있는 문제가 아닐 것 같다고 부정적인 의견을 주셨다.  
주제에 대해서는 좀 더 고민을 해 봐야 할 듯하다.  

>  오늘의 질문 횟수: 3  
>  오늘의 답변 횟수: 3  

+ ##### 키워드: Wide & Deep, DeepFM, DIN, BST, MAB

Day 24 (12/7)
---
오늘은 NGCF를 구현하는 심화 과제를 해결하고, Dive into Deep Learning 교재를 공부했다.  
과제 해결 자체는 한 번에 목표 결과가 나와서 무난했는데, 조교님의 답안 코드와 내가 작성한 코드에 약간 차이가 있었다.  
매 layer에서 이전 layer의 embedding 결과를 사용할 때, 전처리로 정규화를 하느냐 마느냐 하는 차이였다.  
나는 마지막에 layer들의 embedding을 concatenate 할 때 정규화 된 embedding을 연결하길래 이전 layer의 embedding을 불러올 때도 정규화 된 것을 써야한다고 생각했다.  
그래서 이에 대해 질문을 올렸더니 다른 캠퍼분도 의견을 남겨주셔서 같이 고민해봤는데, 원래 논문에서는 이전 layer의 embedding을 그대로 가져오기에 정규화를 하지 않는 게 맞는 것 같기도 하다.  
그렇다면 조교님은 왜 concatenate를 할 때 각 embedding을 정규화한 후에 합치는 걸까? 라는 의문이 들었는데, 이건 아마 layer별 영향력을 비슷하게 유지하기 위함인 듯하다.  

Dive into Deep Learning은 내가 궁금한 부분들 위주로 발췌독을 했다.  
Encoder-Decoder 모델에서 Decoder는 무엇을 입력으로 받는 걸까? 하는 궁금증이 있어서 이를 먼저 찾아봤다.  
찾아보니 Encoder의 output과 state를 이용해 Decoder의 state를 설정하고, Decoder의 이전 출력값을 입력받아 그 다음 출력값을 내놓는 게 기본 구조인 듯하다.  
Transformer를 직접 구현해보면서 구조를 체감하는 것도 재밌을 것 같다.  
그 외에도 분류 문제에서 왜 softmax와 cross-entropy를 사용하는가에 대해서도 공부했다.  
간단히 요약하면 -log P(y|x)를 cross-entropy 함수로 표현할 수 있고, cross-entropy를 모델의 출력 o에 대해 미분하면 softmax(o) - y가 된다.  
따라서 cross-entropy의 미분값을 0으로 만드는 것이 softmax(o)를 y에 맞추는 것과 동일하기에 모델 학습에 사용할 수 있는 것이다.  

피어세션 때는 각자 공부한 내용을 공유하다가 bias-variance tradeoff에 대한 내용이 나왔는데, 얘기를 하다보니 내가 이에 대해 이해가 부족한 느낌이 들었다.  
조원분이 관련 강의 영상 하나를 추천해주셨는데, 이걸 보면서 좀 더 공부해봐야겠다.  

피어세션 후 마스터클래스에서는 커리어 관리에 대한 강연을 들었다.  
회사 생활을 하면서 업무나 직장에 대한 나만의 판단기준을 세워 나가고, 이직이나 부서 이동을 할 때 이 기준에 맞춰 커리어를 쌓아나가야 한다고 하셨다.  
하지만 주니어 때는 이런 기준을 세우기 어렵기에 최대한 여러 회사에 지원해보고, 합격 후에는 하나의 도메인과 직무에서 깊이를 쌓으라고 조언하셨다.  
또 개발자는 개발 역량도 중요하지만, 소통 역량을 키워야 네트워킹을 통해 다양한 정보를 얻고 커리어 관리를 하기 유리하다고 하셨다.  
이전까지의 마스터클래스는 대부분 교수님들이 진행하신 반면, 오늘은 스타트업과 대기업을 옮겨다니신 현직 종사자셔서 더 실용적인 말씀들을 해 주셨다.  

>  오늘의 질문 횟수: 3  
>  오늘의 답변 횟수: 0  

+ ##### 키워드: NGCF, Decoder, Cross-Entropy, 커리어 관리

Day 25 (12/8)
---
오늘은 RecSys 이론 강의 pdf들을 다시 보며 복습하는 시간을 가졌다.  
그동안 캠프 생활을 하면서 복습에 시간을 쓴 적은 거의 없었던 거 같은데, 이번주차 진도를 모두 끝내니 시간이 남아서 복습까지 했다.  

오늘 스페셜 피어세션과 피어세션을 하면서 느낀 건데 RecSys 도메인 캠퍼들 사이에서 내 인지도가 올라가고 있는 것 같다.  
요 근래 슬랙 질문 게시판에 글을 많이 써서 그런 것 같다. 질문 내용들이 좋다고 다들 칭찬해주셔서 자신감을 얻는 중이다.  
스페셜 피어세션 때 내 코딩테스트 결과에 관심을 갖는 분이 계셨다. 캠프에 들어올 때 6.5솔을 했는데, 내가 공부한 기간에 비해 결과가 좋아서 신기해하셨다.  
코테 연습하는 방법을 물어보셨는데, 딱히 비법이라고 생각하는 게 없어서 그냥 꾸준히 문제 풀고 다른 사람 코드 많이 참고하는 게 좋다고 대답했다.  
처음 캠프를 들어올 때는 내가 비전공자에 AI 경험도 없어서 걱정을 많이 했는데, 1달이 좀 더 지난 지금은 제법 잘 해나가고 있다는 게 느껴진다.  

그리고 이번주 팀 회고를 작성할 때 재밌었던 내용이 있어서 사진으로 기록을 남겨두려한다.  

![image](https://github.com/Juniork725/boostcamp/assets/62535139/40c67b93-8c1c-4cdb-8986-69c57322eaf9)

오늘 오프라인 행사에 조원들이 모두 참석했는데 나만 못 가서 아쉬우셨던 거 같다.  
나도 직접 조원들을 못 만난 게 아쉬웠는데, 이런 반응을 보니 고맙기도 하고 재밌었다.  

다만, 오늘 스스로 조금 부족한 점을 느꼈는데 남는 시간이 생길 때 뭘 하면 좋을지 몰라 방황하게 된다는 것이다.  
그동안은 강의와 과제 분량이 빡빡해서 다른 걸 할 여유가 별로 없었는데, 요즘은 스케줄이 조금 느슨해진 느낌이다.  
여유시간에 뭘 할지 생각해 둔 게 없어서 오늘은 고민하다가 복습을 했는데, 앞으로 틈틈이 어떤 걸 해 볼지 생각해둬야겠다.  
당장 생각나는 건 강의에서 다뤘던 논문들의 원문을 읽어보거나, Transformer 같은 모델을 직접 구현해보는 것 정도가 있다.  
둘 다 나름대로 몰입하는 맛이 있을 것 같은데, 특히 모델 구현은 영어 문장을 넣었을 때 한국어로 번역되는 걸 보면 많이 뿌듯할 것 같다.  

>  오늘의 질문 횟수: 0  
>  오늘의 답변 횟수: 0  

+ ##### 키워드: 복습, 사이드 프로젝트트
