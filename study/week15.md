Week 14 (02/13 ~ 02/16)
===
>  ##### 주간 요약
>  

Day 66 (02/13)
---
설날 연휴가 지나고 다시 캠프가 시작된 날이다.  
연휴동안 모델에 자꾸 미련이 남아서 CFM을 구현하고 다듬느라 제대로 쉬질 못한 느낌이다.  
그래서 그런지 간만의 캠프 활동이 꽤나 피곤했다.  
그래도 연휴동안 완성은 거의 한 상태고, 모델이 학습하는 시간이 오래 걸려서 학습을 기다리며 코드를 조금 가다듬는 정도의 작업만 했다.  

확실히 CFM이 기존 FM에 비해서 성능은 잘 나오는 것 같다.  
Multi-hot encoding에 유연하게 대응할 수 있다는 점에서도 이번 프로젝트와 어울리는 부분이 있다.  
다만 그럼에도 RecBole이라는 library의 성능을 뛰어넘기는 힘든 것 같다.  

남은 프로젝트 기간동안은 뭘 더 시도해볼지 고민을 해 봐야겠다.  
지금까지 베이스라인 구축, 모델링 등으로 작업량이 꽤나 많았어서 방전된 느낌도 드는데, 적당히 모델 실험을 돌리면서 아이디어 구상을 하는 시간을 가져야겠다.  

+ ###### CFM 구현, RecBole

Day 67 (02/14)
---
오늘은 그동안 구현해 둔 LCFM과 CFM을 돌려두고, RecBole에서 성능이 잘 나왔던 EASE 모델을 구현해봤다.  
논문을 봤는데 구조 자체가 워낙 간단해서 구현은 어렵지 않았다. 도중에 실수한 부분이 하나 있어서 성능이 제대로 안 나왔는데, 조원분이 리뷰를 통해 찾아주셔서 고쳤다.  
실제로 구현해서 돌려보니 RecBole에서 나오던 성능보다 더 잘 나왔다.  
valid split의 여부나 이미 본 영화를 masking 하는 등의 차이로 인한 결과인 것 같다.  

그리고 LCFM의 경우에는 layer가 깊어졌을 때 기울기 소실 문제가 발생하는 것 같다.  
loss가 자꾸 이상하게 나와서 debugging을 해 보니 모든 입력값에 대해 예측값이 비슷하게 출력되는 문제가 있었다.  
실제로 layer를 줄이니 이러한 문제가 없어졌다.  

기울기 소실이 문제라고 한다면 skip connection이나 batch norm 등을 활용할 수 있을 것 같은데, skip connection은 dimension이 계속 변하는 모델이라 적용이 어려울 듯하다.  
그래서 convolutional layer 사이에 batch norm을 끼워넣는 방안을 우선 고려 중이고, 내일은 leaky ReLU도 넣어서 테스트 해 볼 예정이다.  
다만 EASE 모델 성능이 워낙 좋아서 이걸 뛰어넘기는 힘들 듯하다.  
적당한 성능을 내서 앙상블에 활용하는 수준 정도를 목표로 해야겠다.  

+ ##### EASE 구현, 기울기 소실

Day 68 (02/15)
---
어제 발생했던 기울기 소실 문제를 batch norm으로 해결했다.  
그 외에 skip connection도 적용해보고자 모델의 CNN 구조를 resnet과 유사하게 변경해보고, ReLU도 넣어봤는데 성능 향상이 있지는 않았다.  
처음에는 모델 구조의 복잡도에 비해 데이터의 수가 부족해서 그런가? 라는 생각이 들었는데, 생각해보니 label의 imbalance가 문제일 수도 있을 것 같다.  
다른 조원 분이 Catboost를 돌려서 feature importance를 확인했을 때 user의 importance가 굉장히 높게 나왔다.  
즉, 어떤 유저의 데이터인지만 알아도 label을 어느 정도 예측할 수 있다는 뜻이다.  
영화를 많이 본 유저는 그냥 label을 1로 예측해도 높은 확률로 맞고, 그렇지 않은 유저도 label을 0으로 예측하면 높은 확률로 맞기 때문에 그럴 것이다.  
그래서 내일은 positive sample에 대해 oversampling을 적용해 볼까 싶다.  
사실 이상치 탐지와도 연관성이 있지 않을까? 라는 생각이 들어서 해당 분야에서는 어떻게 불균형을 해결하는지 찾아봤는데, 적용하려면 공부가 좀 필요할 것 같다.  
그래서 우선은 positive sample을 negative sample과 비슷한 양으로 반복 추출하는 방안을 생각 중이다.  

+ ##### Skip Connection, Oversampling
